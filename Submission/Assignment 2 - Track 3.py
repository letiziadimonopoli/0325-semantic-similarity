# -*- coding: utf-8 -*-
"""Assignment 2 - Track 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11FQtCVlKJl11UgzQYiyV4PPbNjHvh4cW
"""

!pip install contractions

import pandas as pd
import numpy as np
import re
import nltk
import spacy
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
from transformers import BertTokenizer
import sentence_transformers
from sentence_transformers import SentenceTransformer
from contractions import fix

train = pd.read_csv("train_responses.csv")
dev = pd.read_csv("dev_responses.csv")
test = pd.read_csv("test_prompts.csv")
train_dev = pd.concat([train, dev], ignore_index=True)

#setting up stopwords
nlp = spacy.load("en_core_web_sm")
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

#same preprocessing as track 2
def clean_text(text):
    text = text.lower().strip()  #lowercase and strip
    text = fix(text) #eg can't becomes cannot
    text = re.sub(f"[{string.punctuation}]", "", text)  #no punct
    text = re.sub(r"\d+", "", text) #no numbers
    doc = nlp(text)
    words = [
        token.lemma_ for token in doc
        if not token.is_punct and not token.is_digit
    ] #lemmatization
    return " ".join(words)

train["user_prompt_clean"] = train["user_prompt"].astype(str).apply(clean_text)
dev["user_prompt_clean"] = dev["user_prompt"].astype(str).apply(clean_text)

sbert_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

#embedding
train_embeddings = sbert_model.encode(train["user_prompt_clean"].tolist(), convert_to_numpy=True)
dev_embeddings = sbert_model.encode(dev["user_prompt_clean"].tolist(), convert_to_numpy=True)

#computing cosine similarity
similarity_matrix = cosine_similarity(dev_embeddings, train_embeddings)
best_match_indices = similarity_matrix.argmax(axis=1)
retrieved_responses = train["model_response"].iloc[best_match_indices].values
dev["retrieved_response"] = retrieved_responses

smoothing_function = SmoothingFunction()

def bleu(x, y):
    if type(x) == str and type(y) == str:
        return sentence_bleu([x.split()], y.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function.method3)
    else: #if it's not a string
        return 0.0

dev["bleu_score"] = dev.apply(lambda x: bleu(x["model_response"], x["retrieved_response"]), axis=1)

print(f"Mean BLEU Score: {dev['bleu_score'].mean():.4f}")

train_dev["user_prompt_clean"] = train_dev["user_prompt"].astype(str).apply(clean_text)
test["user_prompt_clean"] = test["user_prompt"].astype(str).apply(clean_text)

sbert_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

train_dev_embeddings = sbert_model.encode(train_dev["user_prompt_clean"].tolist(), convert_to_numpy=True)
test_embeddings = sbert_model.encode(test["user_prompt_clean"].tolist(), convert_to_numpy=True)

similarity_matrix = cosine_similarity(test_embeddings, train_dev_embeddings)
best_match_indices = similarity_matrix.argmax(axis=1)
test["response_id"] = train_dev.iloc[best_match_indices]["conversation_id"].values
submission = test[["conversation_id", "response_id"]]
submission.to_csv("track_3_test.csv", index=False)

from google.colab import files
files.download('track_3_test.csv')
print("Done")