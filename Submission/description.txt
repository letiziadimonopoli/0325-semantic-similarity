For the first track (“Discrete Text Representation”) after trying multiple approaches, I realized that the best performance was achieved by the TF-IDF vectorizer with the following parameters: analyzer="char", ngram_range=(1, 5). Not much pre-processing was done, except lowering cases and removing leading and trailing whitespaces, as for this track I found that doing more only reduced the BLEU score. The BLEU score I attained with TF-IDF is equivalent to 0.09.

For the second track (“Distributed Static Text Representation”), I realized that doing some preprocessing yielded better results, thus I ended up doing the following: fixing contractions, lower casing, removing punctuation, removing extra spaces, lemmatization and tokenization. Then I decided to use the FastText model as it was the one that performed better, with a BLEU score of 0.08.

Finally, for the third and bonus track (“Open Text Representation”), I again performed the same preprocessing to obtain better results. I decided to implement the following pre-trained model to encode the cleaned text after some other trials: “all-mpnet-base-v2”. Here I also tried computing similarity with dot product similarity and euclidean similarity, but cosine similarity still gave me slightly better results. The BLEU score I obtained in this track is 0.1033.